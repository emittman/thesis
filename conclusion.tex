\chapter{SUMMARY AND CONCLUSION}

We have presented two new applications of Bayesian hierarchical models and our implementations of them, demonstrating both their advantages and their practicality.

Our nonparametric Bayesian model for gene expression was in response to the observation that Bayesian models in use today rely on shrinkage priors that, in some instances, are falsified by the data. That this is true is not surprising. For instance, in a complex experiment, we doubt it reasonable to assume that the true effect sizes within a gene are independent; while it may represent our prior knowledge, we would like to be able update our knowledge after observing many, many genes. Bayesian nonparametrics addresses a problem in parametric statistics, that, even in a parametric hierarchical model, where the hyperparameters are learned from the data, the assumption of a particular parametric family is a strong one, and one that can potentially lead to posteriors which are inconsistent with the underlying truth.

By modeling the effects with a DP, we can make minimal assumptions about the distribution of the gene-by-gene effects. Comparisions of the results of our analysis to competing methods show significant differences in the conclusions that we would tend to draw from the two approaches. We think this is largely due to the small sample sizes we considered, where the number of samples per regression parameter within a gene was $\le 4$. We expect that as sample sizes increase the difference between methods would become negligible. Through experimentation and our simulation studies which we presented in Chapter 3, we expect that the relative performance of our BNP method will depend on the unknown true distributions in a particular experiment. In Simulation Study 1, the ``truth" was constructed using BNP posterior draws and BNP had a substantial advantage in our assessments of thresholded log-fold change. For Simulation Study 2, where the ``truth" was based on voom-limma point estimates, the advantage of BNP was less clear.

A disadvantage of our BNP method is that inference for $\mathcal{P}$ messy, consisting of thousands of weakly identified parameters $(\pi_k,\;\tilde{\beta_k},\; \tilde{\sigma}^2_k)$. This makes it difficult to understand the reasons for the multimodal posterior we observed for certain genes, for example. A different approach which we considered, but did not pursue, would be to model the gene-specific parameters with a \textit{Dirichlet process mixture} by replacing
\begin{equation*}
(\beta_g^\top,\sigma^2_g) \sim \mathcal{P},\quad \mathcal{P} \sim \op{DP}(\alpha Q)
\end{equation*}
with
\begin{equation*}
(\beta_g^\top,\log \sigma_g) \sim \op{MVN}(\mu_g, \Sigma_g),\quad (\mu_g,\Sigma_g) \sim \mathcal{Q},\quad \mathcal{Q} \sim \op{DP}(\alpha R).
\end{equation*}
Effectively, this replaces what was an infinite discrete mixture with an infinite mixture of multivariate normals. This modification would reduce the required number of mixture components by orders of magnitude and thus produce a more parsimonious model fit with smoother and more accountable patterns of shrinkage. We can foresee challenges arising in the implementation of this model; for example, it may be difficult to select and good base measure for $\Sigma_g$.

In Chapter 4, we considered the problem of estimating the lifetime of multiple populations of interest where there is heterogenity arising from infant mortality and where the data available from some populations is severely limited due to truncation and right censoring, factors that are common in reliability field data. Being able to estimate lifetime in these circumstances has great potential value to consumers, as it can inform purchasing decisions.

We addressed the heterogenity within populations through a special kind of mixture model, the GLFP introduced by \citet{chan}, and used a hierarchical model to borrow information across groups. We illustrated our approach with the Backblaze hard drive data where we considered 4 nested models of varying complexity and performing model selection using approximate leave-one-out cross-validation. Our results show that our modeling approach can provide a good fit to complex data.

Future work in this area might consider the consequences of choosing different specifications of the hierarchical model, such as heavy tailed or multivariate distributions, possibly in conjunction with different data coding strategies for purposes of model identification (such as those considered in \citet{chan}). Another possible extension of our approach, noted at the end of Chapter 4, is forecasting the number of units needing replacement over a fixed interval of time, which could improve resource management.

